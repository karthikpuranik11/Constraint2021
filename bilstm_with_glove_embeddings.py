# -*- coding: utf-8 -*-
"""biLSTM with GloVe Embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T5w56EWjQnpkVgG9XP3QCad1OPyLbPx3
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
import re
import matplotlib.pyplot as plt

import pandas as pd
train = pd.read_csv('/content/Constraint_English_Train - Sheet1.csv',#delimiter=',',
                 header=None,names=['id','tweet','label'])
train = train.drop(columns='id')
train= train[1:]
train.label = train.label.apply({'fake':0,'real':1}.get)
train.head(10)

test=pd.read_csv('/content/english_test_with_labels - Sheet1.csv')
test.label = test.label.apply({'fake':0,'real':1}.get)
test

val = pd.read_csv('/content/Constraint_English_Test - Sheet1.csv')
val = val.drop(columns='id')
#val = val[1:]
val['label']=test['label']
val

import nltk
from nltk.tokenize import TweetTokenizer
def tokenizer(df):
    tknzr = TweetTokenizer(strip_handles=True)
    df['tweet']= df['tweet'].apply(lambda x: tknzr.tokenize(x))
    print(df)
    
tokenizer(val)
tokenizer(train)

import re
def clean(df):
    df['tweet']=df['tweet'].apply(lambda x: [i for i in x if not re.match('[^\w\s]',i) and len(i)>3])
    print(df)
clean(train)
clean(val)

nltk.download('stopwords')
nltk.download('wordnet')

from nltk.stem import PorterStemmer
from textblob import Word
st = PorterStemmer()
def stemnlemm(df):
    df['tweet']=df['tweet'].apply(lambda x: [Word(st.stem(i)).lemmatize() for i in x])
    print(df)
stemnlemm(train)
stemnlemm(val)

import nltk
from nltk.corpus import stopwords
stop=stopwords.words("english")
def stop_words(df):
    df['tweet']=df['tweet'].apply(lambda x: [i.lower() for i in x if i not in stop])
    print(df)
stop_words(train)
stop_words(val)

max_features = 2000
max_len = 512
tokenizer = Tokenizer(num_words=max_features, split=' ')
tokenizer.fit_on_texts(train['tweet'].values)
tokenizer.fit_on_texts(val['tweet'].values)
X_train = tokenizer.texts_to_sequences(train['tweet'].values)
X_test = tokenizer.texts_to_sequences(val['tweet'].values)
# vocab_size = len(tokenizer.word_index) + 1
X_train = pad_sequences(X_train,padding = 'post', maxlen=max_len)
X_test = pad_sequences(X_test,padding = 'post', maxlen=max_len)

#X_train=train['tweet']
Y_train=train['label']
#X_test=val['tweet']
Y_test=val['label']
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

from keras.layers import Layer
from keras.layers import Input
from keras.models import Model
from tensorflow.keras import backend as K

class attention(Layer):
    def __init__(self):
        super(attention,self).__init__()

    def build(self,input_shape):
        self.W=self.add_weight(name="att_weight",shape=(input_shape[-1],1),initializer="normal")
        self.b=self.add_weight(name="att_bias",shape=(input_shape[1],1),initializer="zeros")        
        super(attention, self).build(input_shape)

    def call(self,x):
        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)
        at=K.softmax(et)
        at=K.expand_dims(at,axis=-1)
        output=x*at
        return K.sum(output,axis=1)

    def compute_output_shape(self,input_shape):
        return (input_shape[0],input_shape[-1])

    def get_config(self):
        return super(attention,self).get_config()

!wget --header="Host: nlp.stanford.edu" --header="User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-US,en;q=0.9" --header="Cookie: _ga=GA1.2.456156586.1539718115; _gid=GA1.2.491677602.1539718115; _gat=1" --header="Connection: keep-alive" "https://nlp.stanford.edu/data/glove.6B.zip" -O "glove.6B.zip" -c

!unzip glove.6B.zip

from numpy import asarray
 from numpy import zeros

 embeddings_index = dict()

 glove_file = open('glove.6B.100d.txt', encoding="utf8")

 for line in glove_file:
     records = line.split()
     word = records[0]
     vector_dimensions = asarray(records[1:], dtype='float32')
     embeddings_index[word] = vector_dimensions
 glove_file.close()

 print('Found %s word vectors.' %len(embeddings_index))

word_index = tokenizer.word_index
print(len(word_index))


num_words = min(max_features, len(word_index)) + 1
print(num_words)

embedding_dim = 100

embedding_matrix = np.zeros((num_words, embedding_dim))

for word, i in word_index.items():
    if i > max_features:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
    else:
        embedding_matrix[i] = np.random.randn(embedding_dim)

K.clear_session()
from keras.regularizers import l2
from keras.initializers import Constant
embed_dim = 100
lstm_out = 128
# model = Sequential()
inputs = Input(shape=(512,))
x = Embedding(num_words, embed_dim,embeddings_initializer=Constant(embedding_matrix),input_length = X_train.shape[1])(inputs)
att_in = Bidirectional(LSTM(lstm_out,return_sequences=True, dropout=0.2))(x)
att_out = attention()(att_in)
d = Dropout(0.2)(att_out)
outputs = Dense(1, activation='sigmoid')(d)
model = Model(inputs,outputs)
print(model.summary())

model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])

history = model.fit(X_train, Y_train, batch_size = 180, validation_data=(X_test,Y_test), epochs=50, verbose=2)

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy']) 
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'])
#plt.show()
plt.savefig('Model_accuracy.png', dpi=600)

score = model.evaluate(X_test,Y_test,verbose=1)
predictions = np.argmax(model.predict(X_test),axis = -1)

print("Test score is {}".format(score[0]))
print("Test Accuracy is {}".format(score[1]))

_, train_acc = model.evaluate(X_train, Y_train, verbose=0)
_, test_acc = model.evaluate(X_test, Y_test, verbose=0)

rounded_predictions = np.argmax(model.predict(X_test, batch_size=128, verbose=0),axis = -1)

from sklearn.metrics import classification_report
print(classification_report(Y_test, rounded_predictions))

from sklearn.metrics import accuracy_score
score = accuracy_score(Y_test, rounded_predictions)
print(score)

